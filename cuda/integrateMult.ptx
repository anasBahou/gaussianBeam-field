//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_30, debug
.address_size 64

	// .globl	_Z11complexSqrt7double2
.func  (.param .b64 func_retval0) sqrt
(
	.param .b64 sqrt_param_0
)
;
.func sincos
(
	.param .b64 sincos_param_0,
	.param .b64 sincos_param_1,
	.param .b64 sincos_param_2
)
;
.func  (.param .b64 func_retval0) exp
(
	.param .b64 exp_param_0
)
;
.func  (.param .b64 func_retval0) hypot
(
	.param .b64 hypot_param_0,
	.param .b64 hypot_param_1
)
;
.func  (.param .b64 func_retval0) rhypot
(
	.param .b64 rhypot_param_0,
	.param .b64 rhypot_param_1
)
;
.func  (.param .b64 func_retval0) fma
(
	.param .b64 fma_param_0,
	.param .b64 fma_param_1,
	.param .b64 fma_param_2
)
;
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 4 .b8 uDimProd[12];
.const .align 4 .b8 lDim[16];
.const .align 4 .b8 vDim[16];
.const .align 16 .b8 randPhase[16];
.const .align 8 .b8 lMixtureAlpha[256];
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .func  (.param .align 16 .b8 func_retval0[16]) _Z11complexSqrt7double2(
	.param .align 16 .b8 _Z11complexSqrt7double2_param_0[16]
)
{
	.local .align 16 .b8 	__local_depot0[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f64 	%fd<19>;


	.loc 1 60 1
func_begin0:
	.loc	1 0 0

	.loc 1 60 1

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd2, [_Z11complexSqrt7double2_param_0+8];
	ld.param.f64 	%fd1, [_Z11complexSqrt7double2_param_0];
	st.f64 	[%SP+8], %fd2;
	st.f64 	[%SP+0], %fd1;
func_exec_begin0:
	.loc	1 63 5
tmp0:
	ld.f64 	%fd3, [%SP+0];
	ld.f64 	%fd4, [%SP+8];
	.loc	1 63 9
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd3;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd4;
	.param .b64 retval0;
	call.uni (retval0), 
	hypot, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd5, [retval0+0];
	
	//{
	}// Callseq End 0
tmp1:
	.loc	1 64 13
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b64 retval0;
	call.uni (retval0), 
	sqrt, 
	(
	param0
	);
	ld.param.f64	%fd6, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.f64 	%fd7, [%SP+0];
	add.f64 	%fd8, %fd5, %fd7;
	ld.f64 	%fd9, [%SP+8];
	.loc	1 64 23
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd8;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd9;
	.param .b64 retval0;
	call.uni (retval0), 
	rhypot, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd10, [retval0+0];
	
	//{
	}// Callseq End 2
	mul.f64 	%fd11, %fd6, %fd10;
tmp2:
	.loc	1 66 5
	ld.f64 	%fd12, [%SP+0];
	add.f64 	%fd13, %fd12, %fd5;
	mul.f64 	%fd14, %fd11, %fd13;
	st.f64 	[%SP+0], %fd14;
	.loc	1 67 5
	ld.f64 	%fd15, [%SP+8];
	mul.f64 	%fd16, %fd11, %fd15;
	st.f64 	[%SP+8], %fd16;
	.loc	1 68 5
	ld.f64 	%fd17, [%SP+8];
	ld.f64 	%fd18, [%SP+0];
	st.param.f64	[func_retval0+0], %fd18;
	st.param.f64	[func_retval0+8], %fd17;
	ret;
tmp3:
func_end0:
}

	// .globl	_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2_
.visible .entry _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2_(
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_0,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_1,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_2,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_3,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_4,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_5,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_6,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_7,
	.param .u64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_8
)
{
	.local .align 16 .b8 	__local_depot1[528];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .b32 	%r<110>;
	.reg .f64 	%fd<185>;
	.reg .b64 	%rd<56>;


	.loc 1 89 1
func_begin1:
	.loc	1 0 0

	.loc 1 89 1

	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_0];
	ld.param.u64 	%rd2, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_1];
	ld.param.u64 	%rd3, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_2];
	ld.param.u64 	%rd4, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_3];
	ld.param.u64 	%rd5, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_4];
	ld.param.u64 	%rd6, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_5];
	ld.param.u64 	%rd7, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_6];
	ld.param.u64 	%rd8, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_7];
	ld.param.u64 	%rd9, [_Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_8];
func_exec_begin1:
	.loc	1 93 14
tmp4:
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mul.lo.s32 	%r8, %r6, %r7;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
tmp5:
	.loc	1 94 5
	mov.u64 	%rd10, uDimProd;
	cvta.const.u64 	%rd11, %rd10;
	ld.u32 	%r10, [%rd11+8];
	setp.lt.s32	%p1, %r1, %r10;
	not.pred 	%p2, %p1;
	@%p2 bra 	BB1_6;
	bra.uni 	BB1_1;

BB1_1:
tmp6:
	.loc	1 97 9
	mov.u64 	%rd12, uDimProd;
	cvta.const.u64 	%rd13, %rd12;
	ld.u32 	%r11, [%rd13+4];
	div.s32 	%r12, %r1, %r11;
	st.u32 	[%SP+440], %r12;
	.loc	1 98 9
	ld.u32 	%r13, [%SP+440];
	ld.u32 	%r14, [%rd13+4];
	mul.lo.s32 	%r15, %r13, %r14;
	sub.s32 	%r16, %r1, %r15;
	ld.u32 	%r17, [%rd13];
	div.s32 	%r18, %r16, %r17;
	st.u32 	[%SP+436], %r18;
	.loc	1 99 9
	ld.u32 	%r19, [%rd13];
	rem.s32 	%r20, %r1, %r19;
	st.u32 	[%SP+432], %r20;
	.loc	1 100 18
	mov.u64 	%rd14, lDim;
	cvta.const.u64 	%rd15, %rd14;
	ld.u32 	%r21, [%rd15];
	ld.u32 	%r22, [%rd15+4];
	setp.gt.s32	%p3, %r22, 1;
	selp.u32	%r23, 1, 0, %p3;
	mul.lo.s32 	%r24, %r21, %r23;
	ld.u32 	%r25, [%SP+432];
	mul.lo.s32 	%r26, %r24, %r25;
	ld.u32 	%r27, [%rd15];
	ld.u32 	%r28, [%rd15+4];
	mul.lo.s32 	%r29, %r27, %r28;
	ld.u32 	%r30, [%rd15+8];
	setp.gt.s32	%p4, %r30, 1;
	selp.u32	%r31, 1, 0, %p4;
	mul.lo.s32 	%r32, %r29, %r31;
	ld.u32 	%r33, [%SP+436];
	mul.lo.s32 	%r34, %r32, %r33;
	add.s32 	%r35, %r26, %r34;
	ld.u32 	%r36, [%rd15];
	ld.u32 	%r37, [%rd15+4];
	mul.lo.s32 	%r38, %r36, %r37;
	ld.u32 	%r39, [%rd15+8];
	mul.lo.s32 	%r40, %r38, %r39;
	ld.u32 	%r41, [%rd15+12];
	setp.gt.s32	%p5, %r41, 1;
	selp.u32	%r42, 1, 0, %p5;
	mul.lo.s32 	%r43, %r40, %r42;
	ld.u32 	%r44, [%SP+440];
	mul.lo.s32 	%r45, %r43, %r44;
	add.s32 	%r2, %r35, %r45;
tmp7:
	.loc	1 101 18
	mov.u64 	%rd16, vDim;
	cvta.const.u64 	%rd17, %rd16;
	ld.u32 	%r46, [%rd17+4];
	setp.gt.s32	%p6, %r46, 1;
	selp.u32	%r47, 1, 0, %p6;
	ld.u32 	%r48, [%SP+432];
	mul.lo.s32 	%r49, %r47, %r48;
	ld.u32 	%r50, [%rd17+4];
	ld.u32 	%r51, [%rd17+8];
	setp.gt.s32	%p7, %r51, 1;
	selp.u32	%r52, 1, 0, %p7;
	mul.lo.s32 	%r53, %r50, %r52;
	ld.u32 	%r54, [%SP+436];
	mul.lo.s32 	%r55, %r53, %r54;
	add.s32 	%r56, %r49, %r55;
	ld.u32 	%r57, [%rd17+4];
	ld.u32 	%r58, [%rd17+8];
	mul.lo.s32 	%r59, %r57, %r58;
	ld.u32 	%r60, [%rd17+12];
	setp.gt.s32	%p8, %r60, 1;
	selp.u32	%r61, 1, 0, %p8;
	mul.lo.s32 	%r62, %r59, %r61;
	ld.u32 	%r63, [%SP+440];
	mul.lo.s32 	%r64, %r62, %r63;
	add.s32 	%r65, %r56, %r64;
tmp8:
	.loc	1 104 22
	cvt.s64.s32	%rd18, %r65;
	shl.b64 	%rd19, %rd18, 4;
	add.s64 	%rd20, %rd6, %rd19;
	ld.v4.u32 	{%r66, %r67, %r68, %r69}, [%rd20];
	st.v4.u32 	[%SP+464], {%r66, %r67, %r68, %r69};
	.loc	1 105 22
	cvt.s64.s32	%rd21, %r65;
	shl.b64 	%rd22, %rd21, 4;
	add.s64 	%rd23, %rd7, %rd22;
	ld.v4.u32 	{%r74, %r75, %r76, %r77}, [%rd23];
	st.v4.u32 	[%SP+480], {%r74, %r75, %r76, %r77};
	.loc	1 106 22
	cvt.s64.s32	%rd24, %r65;
	shl.b64 	%rd25, %rd24, 4;
	add.s64 	%rd26, %rd8, %rd25;
	ld.v4.u32 	{%r82, %r83, %r84, %r85}, [%rd26];
	st.v4.u32 	[%SP+496], {%r82, %r83, %r84, %r85};
	.loc	1 107 20
	cvt.s64.s32	%rd27, %r65;
	shl.b64 	%rd28, %rd27, 4;
	add.s64 	%rd29, %rd9, %rd28;
	ld.v4.u32 	{%r90, %r91, %r92, %r93}, [%rd29];
	st.v4.u32 	[%SP+512], {%r90, %r91, %r92, %r93};
	.loc	1 109 29
tmp9:
	mov.u32 	%r98, 0;
	mov.b32 	%r3, %r98;
tmp10:
	mov.u32 	%r109, %r3;
tmp11:

BB1_2:
	.loc	1 109 9
	mov.u32 	%r4, %r109;
tmp12:
	mov.u64 	%rd30, lDim;
	cvta.const.u64 	%rd31, %rd30;
	ld.u32 	%r99, [%rd31];
	setp.lt.s32	%p9, %r4, %r99;
	not.pred 	%p10, %p9;
	@%p10 bra 	BB1_5;
	bra.uni 	BB1_3;

BB1_3:
	.loc	1 111 13
tmp13:
	add.s32 	%r100, %r2, %r4;
tmp14:
	.loc	1 112 13
	cvt.s64.s32	%rd32, %r100;
	shl.b64 	%rd33, %rd32, 4;
	add.s64 	%rd34, %rd2, %rd33;
	ld.f64 	%fd1, [%rd34];
	ld.f64 	%fd2, [%rd34+8];
	ld.f64 	%fd3, [%SP+464];
	ld.f64 	%fd4, [%SP+472];
	st.f64 	[%SP+408], %fd2;
	st.f64 	[%SP+400], %fd1;
	st.f64 	[%SP+424], %fd4;
	st.f64 	[%SP+416], %fd3;
	.loc	1 112 48
	bra.uni	tmp15;
tmp15:
	.loc	1 16 5
	ld.f64 	%fd5, [%SP+400];
	ld.f64 	%fd6, [%SP+416];
	add.f64 	%fd7, %fd5, %fd6;
	st.f64 	[%SP+400], %fd7;
	.loc	1 17 5
	ld.f64 	%fd8, [%SP+408];
	ld.f64 	%fd9, [%SP+424];
	add.f64 	%fd10, %fd8, %fd9;
	st.f64 	[%SP+408], %fd10;
	.loc	1 18 5
	ld.f64 	%fd11, [%SP+400];
	ld.f64 	%fd12, [%SP+408];
	st.f64 	[%SP+328], %fd12;
	st.f64 	[%SP+320], %fd11;
tmp16:
	.loc	1 112 34
	bra.uni	tmp17;
tmp17:
	.loc	1 54 28
	ld.f64 	%fd13, [%SP+320];
	mov.f64 	%fd14, %fd13;
tmp18:
	.loc	1 55 5
	ld.f64 	%fd15, [%SP+320];
	ld.f64 	%fd16, [%SP+328];
	sub.f64 	%fd17, %fd15, %fd16;
	ld.f64 	%fd18, [%SP+320];
	ld.f64 	%fd19, [%SP+328];
	add.f64 	%fd20, %fd18, %fd19;
	mul.f64 	%fd21, %fd17, %fd20;
	st.f64 	[%SP+320], %fd21;
	.loc	1 56 5
	mul.f64 	%fd22, %fd14, 0d4000000000000000;
	ld.f64 	%fd23, [%SP+328];
	mul.f64 	%fd24, %fd22, %fd23;
	st.f64 	[%SP+328], %fd24;
	.loc	1 57 5
	ld.f64 	%fd25, [%SP+320];
	ld.f64 	%fd26, [%SP+328];
tmp19:
	.loc	1 112 34
	cvt.s64.s32	%rd35, %r100;
	shl.b64 	%rd36, %rd35, 4;
	add.s64 	%rd37, %rd3, %rd36;
	ld.f64 	%fd27, [%rd37];
	ld.f64 	%fd28, [%rd37+8];
	ld.f64 	%fd29, [%SP+480];
	ld.f64 	%fd30, [%SP+488];
	st.f64 	[%SP+296], %fd28;
	st.f64 	[%SP+288], %fd27;
	st.f64 	[%SP+312], %fd30;
	st.f64 	[%SP+304], %fd29;
	.loc	1 113 31
	bra.uni	tmp20;
tmp20:
	.loc	1 16 5
	ld.f64 	%fd31, [%SP+288];
	ld.f64 	%fd32, [%SP+304];
	add.f64 	%fd33, %fd31, %fd32;
	st.f64 	[%SP+288], %fd33;
	.loc	1 17 5
	ld.f64 	%fd34, [%SP+296];
	ld.f64 	%fd35, [%SP+312];
	add.f64 	%fd36, %fd34, %fd35;
	st.f64 	[%SP+296], %fd36;
	.loc	1 18 5
	ld.f64 	%fd37, [%SP+288];
	ld.f64 	%fd38, [%SP+296];
	st.f64 	[%SP+232], %fd38;
	st.f64 	[%SP+224], %fd37;
tmp21:
	.loc	1 113 17
	bra.uni	tmp22;
tmp22:
	.loc	1 54 28
	ld.f64 	%fd39, [%SP+224];
	mov.f64 	%fd40, %fd39;
tmp23:
	.loc	1 55 5
	ld.f64 	%fd41, [%SP+224];
	ld.f64 	%fd42, [%SP+232];
	sub.f64 	%fd43, %fd41, %fd42;
	ld.f64 	%fd44, [%SP+224];
	ld.f64 	%fd45, [%SP+232];
	add.f64 	%fd46, %fd44, %fd45;
	mul.f64 	%fd47, %fd43, %fd46;
	st.f64 	[%SP+224], %fd47;
	.loc	1 56 5
	mul.f64 	%fd48, %fd40, 0d4000000000000000;
	ld.f64 	%fd49, [%SP+232];
	mul.f64 	%fd50, %fd48, %fd49;
	st.f64 	[%SP+232], %fd50;
	.loc	1 57 5
	ld.f64 	%fd51, [%SP+224];
	ld.f64 	%fd52, [%SP+232];
	st.f64 	[%SP+200], %fd26;
	st.f64 	[%SP+192], %fd25;
	st.f64 	[%SP+216], %fd52;
	st.f64 	[%SP+208], %fd51;
tmp24:
	.loc	1 112 34
	bra.uni	tmp25;
tmp25:
	.loc	1 16 5
	ld.f64 	%fd53, [%SP+192];
	ld.f64 	%fd54, [%SP+208];
	add.f64 	%fd55, %fd53, %fd54;
	st.f64 	[%SP+192], %fd55;
	.loc	1 17 5
	ld.f64 	%fd56, [%SP+200];
	ld.f64 	%fd57, [%SP+216];
	add.f64 	%fd58, %fd56, %fd57;
	st.f64 	[%SP+200], %fd58;
	.loc	1 18 5
	ld.f64 	%fd59, [%SP+192];
	ld.f64 	%fd60, [%SP+200];
tmp26:
	.loc	1 112 34
	cvt.s64.s32	%rd38, %r100;
	shl.b64 	%rd39, %rd38, 4;
	add.s64 	%rd40, %rd4, %rd39;
	ld.f64 	%fd61, [%rd40];
	ld.f64 	%fd62, [%rd40+8];
	ld.f64 	%fd63, [%SP+496];
	ld.f64 	%fd64, [%SP+504];
	st.f64 	[%SP+168], %fd62;
	st.f64 	[%SP+160], %fd61;
	st.f64 	[%SP+184], %fd64;
	st.f64 	[%SP+176], %fd63;
	.loc	1 114 31
	bra.uni	tmp27;
tmp27:
	.loc	1 16 5
	ld.f64 	%fd65, [%SP+160];
	ld.f64 	%fd66, [%SP+176];
	add.f64 	%fd67, %fd65, %fd66;
	st.f64 	[%SP+160], %fd67;
	.loc	1 17 5
	ld.f64 	%fd68, [%SP+168];
	ld.f64 	%fd69, [%SP+184];
	add.f64 	%fd70, %fd68, %fd69;
	st.f64 	[%SP+168], %fd70;
	.loc	1 18 5
	ld.f64 	%fd71, [%SP+160];
	ld.f64 	%fd72, [%SP+168];
	st.f64 	[%SP+120], %fd72;
	st.f64 	[%SP+112], %fd71;
tmp28:
	.loc	1 114 17
	bra.uni	tmp29;
tmp29:
	.loc	1 54 28
	ld.f64 	%fd73, [%SP+112];
	mov.f64 	%fd74, %fd73;
tmp30:
	.loc	1 55 5
	ld.f64 	%fd75, [%SP+112];
	ld.f64 	%fd76, [%SP+120];
	sub.f64 	%fd77, %fd75, %fd76;
	ld.f64 	%fd78, [%SP+112];
	ld.f64 	%fd79, [%SP+120];
	add.f64 	%fd80, %fd78, %fd79;
	mul.f64 	%fd81, %fd77, %fd80;
	st.f64 	[%SP+112], %fd81;
	.loc	1 56 5
	mul.f64 	%fd82, %fd74, 0d4000000000000000;
	ld.f64 	%fd83, [%SP+120];
	mul.f64 	%fd84, %fd82, %fd83;
	st.f64 	[%SP+120], %fd84;
	.loc	1 57 5
	ld.f64 	%fd85, [%SP+112];
	ld.f64 	%fd86, [%SP+120];
	st.f64 	[%SP+88], %fd60;
	st.f64 	[%SP+80], %fd59;
	st.f64 	[%SP+104], %fd86;
	st.f64 	[%SP+96], %fd85;
tmp31:
	.loc	1 112 34
	bra.uni	tmp32;
tmp32:
	.loc	1 16 5
	ld.f64 	%fd87, [%SP+80];
	ld.f64 	%fd88, [%SP+96];
	add.f64 	%fd89, %fd87, %fd88;
	st.f64 	[%SP+80], %fd89;
	.loc	1 17 5
	ld.f64 	%fd90, [%SP+88];
	ld.f64 	%fd91, [%SP+104];
	add.f64 	%fd92, %fd90, %fd91;
	st.f64 	[%SP+88], %fd92;
	.loc	1 18 5
	ld.f64 	%fd93, [%SP+88];
	ld.f64 	%fd94, [%SP+80];
tmp33:
	.loc	1 112 22
	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .align 16 .b8 param0[16];
	st.param.f64	[param0+0], %fd94;
	st.param.f64	[param0+8], %fd93;
	.param .align 16 .b8 retval0[16];
	call.uni (retval0), 
	_Z11complexSqrt7double2, 
	(
	param0
	);
	ld.param.f64	%fd95, [retval0+0];
	ld.param.f64	%fd96, [retval0+8];
	
	//{
	}// Callseq End 3
	st.f64 	[%SP+456], %fd96;
	st.f64 	[%SP+448], %fd95;
	.loc	1 115 13
	cvt.s64.s32	%rd41, %r1;
	shl.b64 	%rd42, %rd41, 4;
	add.s64 	%rd43, %rd1, %rd42;
tmp34:
	mov.u64 	%rd44, lMixtureAlpha;
	cvta.const.u64 	%rd45, %rd44;
	cvt.s64.s32	%rd46, %r4;
	shl.b64 	%rd47, %rd46, 3;
	add.s64 	%rd48, %rd45, %rd47;
	ld.f64 	%fd97, [%rd48];
	mov.u64 	%rd49, randPhase;
	cvta.const.u64 	%rd50, %rd49;
	ld.f64 	%fd98, [%rd50];
	ld.f64 	%fd99, [%rd50+8];
	mov.f64 	%fd100, %fd97;
tmp35:
	st.f64 	[%SP+8], %fd99;
	st.f64 	[%SP+0], %fd98;
	.loc	1 115 40
	bra.uni	tmp36;
tmp36:
	.loc	1 38 5
	ld.f64 	%fd101, [%SP+0];
	mul.f64 	%fd102, %fd101, %fd100;
	st.f64 	[%SP+0], %fd102;
	.loc	1 39 5
	ld.f64 	%fd103, [%SP+8];
	mul.f64 	%fd104, %fd103, %fd100;
	st.f64 	[%SP+8], %fd104;
	.loc	1 40 5
	ld.f64 	%fd105, [%SP+0];
	ld.f64 	%fd106, [%SP+8];
tmp37:
	.loc	1 115 40
	cvt.s64.s32	%rd51, %r100;
	shl.b64 	%rd52, %rd51, 4;
	add.s64 	%rd53, %rd5, %rd52;
	ld.f64 	%fd107, [%rd53];
	ld.f64 	%fd108, [%rd53+8];
	ld.f64 	%fd109, [%SP+512];
	ld.f64 	%fd110, [%SP+520];
	st.f64 	[%SP+24], %fd108;
	st.f64 	[%SP+16], %fd107;
	st.f64 	[%SP+40], %fd110;
	st.f64 	[%SP+32], %fd109;
	.loc	1 115 98
	bra.uni	tmp38;
tmp38:
	.loc	1 16 5
	ld.f64 	%fd111, [%SP+16];
	ld.f64 	%fd112, [%SP+32];
	add.f64 	%fd113, %fd111, %fd112;
	st.f64 	[%SP+16], %fd113;
	.loc	1 17 5
	ld.f64 	%fd114, [%SP+24];
	ld.f64 	%fd115, [%SP+40];
	add.f64 	%fd116, %fd114, %fd115;
	st.f64 	[%SP+24], %fd116;
	.loc	1 18 5
	ld.f64 	%fd117, [%SP+16];
	ld.f64 	%fd118, [%SP+24];
tmp39:
	.loc	1 115 98
	ld.f64 	%fd119, [%SP+448];
	ld.f64 	%fd120, [%SP+456];
	st.f64 	[%SP+56], %fd118;
	st.f64 	[%SP+48], %fd117;
	st.f64 	[%SP+72], %fd120;
	st.f64 	[%SP+64], %fd119;
	.loc	1 115 98
	bra.uni	tmp40;
tmp40:
	.loc	1 16 5
	ld.f64 	%fd121, [%SP+48];
	ld.f64 	%fd122, [%SP+64];
	add.f64 	%fd123, %fd121, %fd122;
	st.f64 	[%SP+48], %fd123;
	.loc	1 17 5
	ld.f64 	%fd124, [%SP+56];
	ld.f64 	%fd125, [%SP+72];
	add.f64 	%fd126, %fd124, %fd125;
	st.f64 	[%SP+56], %fd126;
	.loc	1 18 5
	ld.f64 	%fd127, [%SP+48];
	ld.f64 	%fd128, [%SP+56];
	st.f64 	[%SP+136], %fd128;
	st.f64 	[%SP+128], %fd127;
tmp41:
	.loc	1 115 82
	bra.uni	tmp42;
tmp42:
	.loc	1 74 5
	ld.f64 	%fd129, [%SP+128];
	.loc	1 74 12
	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd129;
	.param .b64 retval0;
	call.uni (retval0), 
	exp, 
	(
	param0
	);
	ld.param.f64	%fd130, [retval0+0];
	
	//{
	}// Callseq End 4
tmp43:
	.loc	1 75 5
	ld.f64 	%fd131, [%SP+136];
	add.u64 	%rd54, %SP, 144;
tmp44:
	add.u64 	%rd55, %SP, 152;
tmp45:
	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd131;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd54;
	.param .b64 param2;
	st.param.b64	[param2+0], %rd55;
	call.uni 
	sincos, 
	(
	param0, 
	param1, 
	param2
	);
	
	//{
	}// Callseq End 5
	.loc	1 76 5
	ld.f64 	%fd132, [%SP+152];
	mul.f64 	%fd133, %fd130, %fd132;
	st.f64 	[%SP+128], %fd133;
	.loc	1 77 5
	ld.f64 	%fd134, [%SP+144];
	mul.f64 	%fd135, %fd130, %fd134;
	st.f64 	[%SP+136], %fd135;
	.loc	1 78 5
	ld.f64 	%fd136, [%SP+128];
	ld.f64 	%fd137, [%SP+136];
tmp46:
	.loc	1 115 82
	ld.f64 	%fd138, [%SP+448];
	ld.f64 	%fd139, [%SP+456];
	st.f64 	[%SP+248], %fd137;
	st.f64 	[%SP+240], %fd136;
	st.f64 	[%SP+264], %fd139;
	st.f64 	[%SP+256], %fd138;
	.loc	1 115 82
	bra.uni	tmp47;
tmp47:
	.loc	1 30 24
	ld.f64 	%fd140, [%SP+256];
	ld.f64 	%fd141, [%SP+256];
	ld.f64 	%fd142, [%SP+264];
	ld.f64 	%fd143, [%SP+264];
	mul.f64 	%fd144, %fd142, %fd143;
	.loc	1 30 29
	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd140;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd141;
	.param .b64 param2;
	st.param.f64	[param2+0], %fd144;
	.param .b64 retval0;
	call.uni (retval0), 
	fma, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.f64	%fd145, [retval0+0];
	
	//{
	}// Callseq End 6
	rcp.rn.f64 	%fd146, %fd145;
tmp48:
	.loc	1 31 5
	ld.f64 	%fd147, [%SP+240];
	ld.f64 	%fd148, [%SP+256];
	ld.f64 	%fd149, [%SP+248];
	ld.f64 	%fd150, [%SP+264];
	mul.f64 	%fd151, %fd149, %fd150;
	.loc	1 31 12
	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd147;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd148;
	.param .b64 param2;
	st.param.f64	[param2+0], %fd151;
	.param .b64 retval0;
	call.uni (retval0), 
	fma, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.f64	%fd152, [retval0+0];
	
	//{
	}// Callseq End 7
	mul.f64 	%fd153, %fd152, %fd146;
	st.f64 	[%SP+272], %fd153;
	.loc	1 32 5
	ld.f64 	%fd154, [%SP+248];
	ld.f64 	%fd155, [%SP+256];
	ld.f64 	%fd156, [%SP+240];
	neg.f64 	%fd157, %fd156;
	ld.f64 	%fd158, [%SP+264];
	mul.f64 	%fd159, %fd157, %fd158;
	.loc	1 32 12
	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd154;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd155;
	.param .b64 param2;
	st.param.f64	[param2+0], %fd159;
	.param .b64 retval0;
	call.uni (retval0), 
	fma, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.f64	%fd160, [retval0+0];
	
	//{
	}// Callseq End 8
	mul.f64 	%fd161, %fd160, %fd146;
	st.f64 	[%SP+280], %fd161;
	.loc	1 33 5
	ld.f64 	%fd162, [%SP+272];
	ld.f64 	%fd163, [%SP+280];
	st.f64 	[%SP+344], %fd106;
	st.f64 	[%SP+336], %fd105;
	st.f64 	[%SP+360], %fd163;
	st.f64 	[%SP+352], %fd162;
tmp49:
	.loc	1 115 39
	bra.uni	tmp50;
tmp50:
	.loc	1 45 29
	ld.v4.u32 	{%r101, %r102, %r103, %r104}, [%SP+336];
	st.v4.u32 	[%SP+368], {%r101, %r102, %r103, %r104};
	.loc	1 47 5
	ld.f64 	%fd164, [%SP+336];
	ld.f64 	%fd165, [%SP+352];
	ld.f64 	%fd166, [%SP+344];
	neg.f64 	%fd167, %fd166;
	ld.f64 	%fd168, [%SP+360];
	mul.f64 	%fd169, %fd167, %fd168;
	.loc	1 47 11
	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd164;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd165;
	.param .b64 param2;
	st.param.f64	[param2+0], %fd169;
	.param .b64 retval0;
	call.uni (retval0), 
	fma, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.f64	%fd170, [retval0+0];
	
	//{
	}// Callseq End 9
	st.f64 	[%SP+336], %fd170;
	.loc	1 48 5
	ld.f64 	%fd171, [%SP+368];
	ld.f64 	%fd172, [%SP+360];
	ld.f64 	%fd173, [%SP+376];
	ld.f64 	%fd174, [%SP+352];
	mul.f64 	%fd175, %fd173, %fd174;
	.loc	1 48 11
	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd171;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd172;
	.param .b64 param2;
	st.param.f64	[param2+0], %fd175;
	.param .b64 retval0;
	call.uni (retval0), 
	fma, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.f64	%fd176, [retval0+0];
	
	//{
	}// Callseq End 10
	st.f64 	[%SP+344], %fd176;
	.loc	1 49 5
	ld.f64 	%fd177, [%SP+336];
	ld.f64 	%fd178, [%SP+344];
	st.f64 	[%SP+392], %fd178;
	st.f64 	[%SP+384], %fd177;
tmp51:
	.loc	1 115 13
	bra.uni	tmp52;
tmp52:
	.loc	1 85 5
	ld.f64 	%fd179, [%SP+384];
	ld.f64 	%fd180, [%rd43];
	add.f64 	%fd181, %fd180, %fd179;
	st.f64 	[%rd43], %fd181;
	.loc	1 86 5
	ld.f64 	%fd182, [%SP+392];
	ld.f64 	%fd183, [%rd43+8];
	add.f64 	%fd184, %fd183, %fd182;
	st.f64 	[%rd43+8], %fd184;
tmp53:

	.loc	1 109 56
	add.s32 	%r5, %r4, 1;
tmp54:
	mov.u32 	%r109, %r5;
tmp55:
	bra.uni 	BB1_2;
tmp56:

BB1_5:

BB1_6:
	.loc	1 119 1
	ret;
tmp57:
func_end1:
}

.func  (.param .b64 func_retval0) sqrt(
	.param .b64 sqrt_param_0
)
{
	.reg .f64 	%fd<3>;


	ld.param.f64 	%fd1, [sqrt_param_0];
	sqrt.rn.f64 	%fd2, %fd1;
	st.param.f64	[func_retval0+0], %fd2;
	ret;
}

.func sincos(
	.param .b64 sincos_param_0,
	.param .b64 sincos_param_1,
	.param .b64 sincos_param_2
)
{
	.local .align 4 .b8 	__local_depot3[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<14>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<62>;
	.reg .b64 	%rd<4>;


	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd15, [sincos_param_0];
	ld.param.u64 	%rd1, [sincos_param_1];
	ld.param.u64 	%rd2, [sincos_param_2];
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1, %temp}, %fd15;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd15;
	}
	and.b32  	%r4, %r3, 2147483647;
	setp.eq.s32	%p4, %r4, 2146435072;
	mov.pred 	%p3, 0;
	not.pred 	%p5, %p4;
	mov.pred 	%p13, %p3;
	@%p5 bra 	BB3_2;
	bra.uni 	BB3_1;

BB3_1:
	setp.eq.s32	%p1, %r1, 0;
	mov.pred 	%p13, %p1;

BB3_2:
	mov.pred 	%p2, %p13;
	not.pred 	%p6, %p2;
	mov.f64 	%fd56, %fd15;
	@%p6 bra 	BB3_4;
	bra.uni 	BB3_3;

BB3_3:
	mov.f64 	%fd16, 0d0000000000000000;
	mul.rn.f64 	%fd1, %fd15, %fd16;
	mov.f64 	%fd56, %fd1;

BB3_4:
	mov.f64 	%fd2, %fd56;
	mul.f64 	%fd17, %fd2, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r5, %fd17;
	st.u32 	[%SP+0], %r5;
	ld.u32 	%r6, [%SP+0];
	cvt.rn.f64.s32	%fd18, %r6;
	neg.f64 	%fd19, %fd18;
	mov.f64 	%fd20, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd21, %fd19, %fd20, %fd2;
	neg.f64 	%fd22, %fd18;
	mov.f64 	%fd23, 0d3C91A62633145C00;
	fma.rn.f64 	%fd24, %fd22, %fd23, %fd21;
	neg.f64 	%fd25, %fd18;
	mov.f64 	%fd26, 0d397B839A252049C0;
	fma.rn.f64 	%fd3, %fd25, %fd26, %fd24;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd2;
	}
	and.b32  	%r8, %r7, 2147483647;
	setp.ge.s32	%p7, %r8, 1105199104;
	not.pred 	%p8, %p7;
	mov.f64 	%fd57, %fd3;
	@%p8 bra 	BB3_6;
	bra.uni 	BB3_5;

BB3_5:
	add.u64 	%rd3, %SP, 0;
	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd2;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd3;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd4, [retval0+0];
	
	//{
	}// Callseq End 11
	mov.f64 	%fd57, %fd4;

BB3_6:
	mov.f64 	%fd5, %fd57;
	ld.u32 	%r2, [%SP+0];
	mul.rn.f64 	%fd27, %fd5, %fd5;
	mov.f64 	%fd28, 0d3E21EEA7C1EF8528;
	mov.f64 	%fd29, 0dBDA8FF8320FD8164;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0dBE927E4F8E06E6D9;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3EFA01A019DDBCE9;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0dBF56C16C16C15D47;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0d3FA5555555555551;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0dBFE0000000000000;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	mov.f64 	%fd41, 0d3FF0000000000000;
	fma.rn.f64 	%fd6, %fd40, %fd27, %fd41;
	mul.rn.f64 	%fd42, %fd5, %fd5;
	mov.f64 	%fd43, 0dBE5AE5F12CB0D246;
	mov.f64 	%fd44, 0d3DE5DB65F9785EBA;
	fma.rn.f64 	%fd45, %fd44, %fd42, %fd43;
	mov.f64 	%fd46, 0d3EC71DE369ACE392;
	fma.rn.f64 	%fd47, %fd45, %fd42, %fd46;
	mov.f64 	%fd48, 0dBF2A01A019DB62A1;
	fma.rn.f64 	%fd49, %fd47, %fd42, %fd48;
	mov.f64 	%fd50, 0d3F81111111110818;
	fma.rn.f64 	%fd51, %fd49, %fd42, %fd50;
	mov.f64 	%fd52, 0dBFC5555555555554;
	fma.rn.f64 	%fd53, %fd51, %fd42, %fd52;
	mov.f64 	%fd54, 0d0000000000000000;
	fma.rn.f64 	%fd55, %fd53, %fd42, %fd54;
	fma.rn.f64 	%fd7, %fd55, %fd5, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd7;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd7;
	}
	xor.b32  	%r11, %r9, -2147483648;
	mov.b64 	%fd8, {%r10, %r11};
	and.b32  	%r12, %r2, 1;
	setp.ne.s32	%p9, %r12, 0;
	not.pred 	%p10, %p9;
	mov.f64 	%fd58, %fd7;
	mov.f64 	%fd59, %fd6;
	@%p10 bra 	BB3_8;
	bra.uni 	BB3_7;

BB3_7:
	mov.f64 	%fd58, %fd6;
	mov.f64 	%fd59, %fd8;

BB3_8:
	mov.f64 	%fd10, %fd59;
	mov.f64 	%fd9, %fd58;
	and.b32  	%r13, %r2, 2;
	setp.ne.s32	%p11, %r13, 0;
	not.pred 	%p12, %p11;
	mov.f64 	%fd60, %fd9;
	mov.f64 	%fd61, %fd10;
	@%p12 bra 	BB3_10;
	bra.uni 	BB3_9;

BB3_9:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd9;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd9;
	}
	xor.b32  	%r16, %r14, -2147483648;
	mov.b64 	%fd11, {%r15, %r16};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd10;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd10;
	}
	xor.b32  	%r19, %r17, -2147483648;
	mov.b64 	%fd12, {%r18, %r19};
	mov.f64 	%fd60, %fd11;
	mov.f64 	%fd61, %fd12;

BB3_10:
	mov.f64 	%fd14, %fd61;
	mov.f64 	%fd13, %fd60;
	st.f64 	[%rd1], %fd13;
	st.f64 	[%rd2], %fd14;
	ret;
}

.func  (.param .b64 func_retval0) exp(
	.param .b64 exp_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<17>;
	.reg .f64 	%fd<46>;


	ld.param.f64 	%fd8, [exp_param_0];
	mov.f64 	%fd9, 0d4338000000000000;
	mov.f64 	%fd10, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd11, %fd8, %fd10, %fd9;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1, %temp}, %fd11;
	}
	fma.rn.f64 	%fd12, %fd8, %fd10, %fd9;
	mov.f64 	%fd13, 0dC338000000000000;
	add.rn.f64 	%fd14, %fd12, %fd13;
	mov.f64 	%fd15, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd16, %fd14, %fd15, %fd8;
	mov.f64 	%fd17, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd18, %fd14, %fd17, %fd16;
	mov.f64 	%fd19, 0d3E928AF3FCA213EA;
	mov.f64 	%fd20, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	mov.f64 	%fd22, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd23, %fd21, %fd18, %fd22;
	mov.f64 	%fd24, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd25, %fd23, %fd18, %fd24;
	mov.f64 	%fd26, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd27, %fd25, %fd18, %fd26;
	mov.f64 	%fd28, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd29, %fd27, %fd18, %fd28;
	mov.f64 	%fd30, 0d3F81111111122322;
	fma.rn.f64 	%fd31, %fd29, %fd18, %fd30;
	mov.f64 	%fd32, 0d3FA55555555502A1;
	fma.rn.f64 	%fd33, %fd31, %fd18, %fd32;
	mov.f64 	%fd34, 0d3FC5555555555511;
	fma.rn.f64 	%fd35, %fd33, %fd18, %fd34;
	mov.f64 	%fd36, 0d3FE000000000000B;
	fma.rn.f64 	%fd37, %fd35, %fd18, %fd36;
	mov.f64 	%fd38, 0d3FF0000000000000;
	fma.rn.f64 	%fd39, %fd37, %fd18, %fd38;
	fma.rn.f64 	%fd1, %fd39, %fd18, %fd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd1;
	}
	shl.b32 	%r4, %r1, 20;
	add.s32 	%r5, %r4, %r3;
	mov.b64 	%fd2, {%r2, %r5};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r6}, %fd8;
	}
	mov.b32 	 %f1, %r6;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p1, %f2, 0f4086232B;
	not.pred 	%p2, %p1;
	not.pred 	%p3, %p2;
	mov.f64 	%fd45, %fd2;
	@%p3 bra 	BB4_7;
	bra.uni 	BB4_1;

BB4_1:
	setp.lt.f64	%p4, %fd8, 0d0000000000000000;
	not.pred 	%p5, %p4;
	@%p5 bra 	BB4_3;
	bra.uni 	BB4_2;

BB4_2:
	mov.f64 	%fd40, 0d0000000000000000;
	mov.f64 	%fd43, %fd40;
	bra.uni 	BB4_4;

BB4_3:
	add.f64 	%fd3, %fd8, 0d7FF0000000000000;
	mov.f64 	%fd43, %fd3;

BB4_4:
	mov.f64 	%fd4, %fd43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd8;
	}
	mov.b32 	 %f3, %r7;
	abs.f32 	%f4, %f3;
	setp.lt.f32	%p6, %f4, 0f40874800;
	not.pred 	%p7, %p6;
	mov.f64 	%fd44, %fd4;
	@%p7 bra 	BB4_6;
	bra.uni 	BB4_5;

BB4_5:
	div.s32 	%r8, %r1, 2;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r9, %temp}, %fd1;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r10}, %fd1;
	}
	shl.b32 	%r11, %r8, 20;
	add.s32 	%r12, %r10, %r11;
	mov.b64 	%fd41, {%r9, %r12};
	sub.s32 	%r13, %r1, %r8;
	shl.b32 	%r14, %r13, 20;
	add.s32 	%r15, %r14, 1072693248;
	mov.u32 	%r16, 0;
	mov.b64 	%fd42, {%r16, %r15};
	mul.f64 	%fd5, %fd41, %fd42;
	mov.f64 	%fd44, %fd5;

BB4_6:
	mov.f64 	%fd6, %fd44;
	mov.f64 	%fd45, %fd6;

BB4_7:
	mov.f64 	%fd7, %fd45;
	st.param.f64	[func_retval0+0], %fd7;
	ret;
}

.func  (.param .b64 func_retval0) hypot(
	.param .b64 hypot_param_0,
	.param .b64 hypot_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<11>;
	.reg .f64 	%fd<32>;
	.reg .b64 	%rd<7>;


	ld.param.f64 	%fd6, [hypot_param_0];
	ld.param.f64 	%fd7, [hypot_param_1];
	abs.f64 	%fd10, %fd6;
	abs.f64 	%fd11, %fd7;
	mov.b64 	 %rd1, %fd11;
	mov.b64 	 %rd2, %fd10;
	min.u64 	%rd3, %rd1, %rd2;
	mov.b64 	 %fd1, %rd3;
	mov.b64 	 %rd4, %fd10;
	mov.b64 	 %rd5, %fd11;
	max.u64 	%rd6, %rd4, %rd5;
	mov.b64 	 %fd2, %rd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd2;
	}
	and.b32  	%r2, %r1, -4194304;
	sub.s32 	%r3, %r2, 1072693248;
	mov.u32 	%r4, 1072693248;
	add.s32 	%r5, %r3, 1048576;
	sub.s32 	%r6, %r4, %r5;
	mov.u32 	%r7, 0;
	mov.b64 	%fd12, {%r7, %r6};
	mul.f64 	%fd13, %fd1, %fd12;
	mul.f64 	%fd14, %fd2, %fd12;
	mul.f64 	%fd15, %fd13, %fd13;
	fma.rn.f64 	%fd16, %fd14, %fd14, %fd15;
	mov.f64 	%fd17, 0d7FEFFFFFFFFFFFFF;
	min.f64 	%fd9, %fd16, %fd17;
	// inline asm
	rsqrt.approx.ftz.f64 %fd8, %fd9;
	// inline asm
	mul.rn.f64 	%fd18, %fd8, %fd8;
	neg.f64 	%fd19, %fd18;
	mov.f64 	%fd20, 0d3FF0000000000000;
	fma.rn.f64 	%fd21, %fd9, %fd19, %fd20;
	mov.f64 	%fd22, 0d3FE0000000000000;
	mov.f64 	%fd23, 0d3FD8000000000000;
	fma.rn.f64 	%fd24, %fd23, %fd21, %fd22;
	mul.rn.f64 	%fd25, %fd21, %fd8;
	fma.rn.f64 	%fd26, %fd24, %fd25, %fd8;
	mul.f64 	%fd27, %fd16, %fd26;
	add.s32 	%r8, %r5, 1072693248;
	mov.b64 	%fd28, {%r7, %r8};
	mul.f64 	%fd3, %fd27, %fd28;
	setp.eq.f64	%p1, %fd1, 0d0000000000000000;
	not.pred 	%p2, %p1;
	mov.f64 	%fd30, %fd3;
	@%p2 bra 	BB5_2;
	bra.uni 	BB5_1;

BB5_1:
	mov.f64 	%fd30, %fd2;

BB5_2:
	mov.f64 	%fd4, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd1;
	}
	mov.f64 	%fd29, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r10}, %fd29;
	}
	setp.ge.u32	%p3, %r9, %r10;
	not.pred 	%p4, %p3;
	mov.f64 	%fd31, %fd4;
	@%p4 bra 	BB5_4;
	bra.uni 	BB5_3;

BB5_3:
	mov.f64 	%fd31, %fd1;

BB5_4:
	mov.f64 	%fd5, %fd31;
	st.param.f64	[func_retval0+0], %fd5;
	ret;
}

.func  (.param .b64 func_retval0) rhypot(
	.param .b64 rhypot_param_0,
	.param .b64 rhypot_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<14>;
	.reg .f64 	%fd<35>;
	.reg .b64 	%rd<7>;


	ld.param.f64 	%fd9, [rhypot_param_0];
	ld.param.f64 	%fd10, [rhypot_param_1];
	abs.f64 	%fd13, %fd9;
	abs.f64 	%fd14, %fd10;
	mov.b64 	 %rd1, %fd14;
	mov.b64 	 %rd2, %fd13;
	min.u64 	%rd3, %rd1, %rd2;
	mov.b64 	 %fd1, %rd3;
	mov.b64 	 %rd4, %fd13;
	mov.b64 	 %rd5, %fd14;
	max.u64 	%rd6, %rd4, %rd5;
	mov.b64 	 %fd2, %rd6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd2;
	}
	and.b32  	%r2, %r1, -4194304;
	sub.s32 	%r3, %r2, 1072693248;
	mov.u32 	%r4, 1072693248;
	add.s32 	%r5, %r3, 1048576;
	sub.s32 	%r6, %r4, %r5;
	mov.u32 	%r7, 0;
	mov.b64 	%fd15, {%r7, %r6};
	mul.f64 	%fd16, %fd1, %fd15;
	mul.f64 	%fd17, %fd2, %fd15;
	mul.f64 	%fd18, %fd16, %fd16;
	fma.rn.f64 	%fd12, %fd17, %fd17, %fd18;
	// inline asm
	rsqrt.approx.ftz.f64 %fd11, %fd12;
	// inline asm
	mul.rn.f64 	%fd19, %fd11, %fd11;
	neg.f64 	%fd20, %fd19;
	mov.f64 	%fd21, 0d3FF0000000000000;
	fma.rn.f64 	%fd22, %fd12, %fd20, %fd21;
	mov.f64 	%fd23, 0d3FE0000000000000;
	mov.f64 	%fd24, 0d3FD8000000000000;
	fma.rn.f64 	%fd25, %fd24, %fd22, %fd23;
	mul.rn.f64 	%fd26, %fd22, %fd11;
	fma.rn.f64 	%fd27, %fd25, %fd26, %fd11;
	mul.f64 	%fd3, %fd27, %fd15;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd3;
	}
	mov.f64 	%fd28, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd28;
	}
	setp.ge.u32	%p1, %r8, %r9;
	not.pred 	%p2, %p1;
	mov.f64 	%fd34, %fd3;
	@%p2 bra 	BB6_7;
	bra.uni 	BB6_1;

BB6_1:
	abs.f64 	%fd29, %fd2;
	setp.le.f64	%p3, %fd29, 0d7FF0000000000000;
	not.pred 	%p4, %p3;
	not.pred 	%p5, %p4;
	@%p5 bra 	BB6_3;
	bra.uni 	BB6_2;

BB6_2:
	add.f64 	%fd4, %fd2, %fd1;
	mov.f64 	%fd32, %fd4;
	bra.uni 	BB6_4;

BB6_3:
	mov.f64 	%fd30, 0d7FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r10}, %fd30;
	}
	setp.neu.f64	%p6, %fd2, 0d7FF0000000000000;
	selp.u32	%r11, 1, 0, %p6;
	mul.lo.s32 	%r12, %r10, %r11;
	mov.u32 	%r13, 0;
	mov.b64 	%fd5, {%r13, %r12};
	mov.f64 	%fd32, %fd5;

BB6_4:
	mov.f64 	%fd6, %fd32;
	setp.eq.f64	%p7, %fd1, 0d7FF0000000000000;
	not.pred 	%p8, %p7;
	mov.f64 	%fd33, %fd6;
	@%p8 bra 	BB6_6;
	bra.uni 	BB6_5;

BB6_5:
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd33, %fd31;

BB6_6:
	mov.f64 	%fd7, %fd33;
	mov.f64 	%fd34, %fd7;

BB6_7:
	mov.f64 	%fd8, %fd34;
	st.param.f64	[func_retval0+0], %fd8;
	ret;
}

.func  (.param .b64 func_retval0) fma(
	.param .b64 fma_param_0,
	.param .b64 fma_param_1,
	.param .b64 fma_param_2
)
{
	.reg .f64 	%fd<5>;


	ld.param.f64 	%fd1, [fma_param_0];
	ld.param.f64 	%fd2, [fma_param_1];
	ld.param.f64 	%fd3, [fma_param_2];
	fma.rn.f64 	%fd4, %fd1, %fd2, %fd3;
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 16 .b8 	__local_depot8[128];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<40>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<95>;


	mov.u64 	%SPL, __local_depot8;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd3, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd20, [__internal_trig_reduction_slowpathd_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r18}, %fd3;
	}
	and.b32  	%r1, %r18, -2147483648;
	shr.u32 	%r19, %r18, 20;
	and.b32  	%r2, %r19, 2047;
	setp.eq.s32	%p1, %r2, 2047;
	not.pred 	%p2, %p1;
	@%p2 bra 	BB8_2;
	bra.uni 	BB8_1;

BB8_1:
	mov.f64 	%fd4, %fd3;
	bra.uni 	BB8_17;

BB8_2:
	sub.s32 	%r3, %r2, 1024;
	mov.b64 	 %rd21, %fd3;
	shl.b64 	%rd22, %rd21, 11;
	or.b64  	%rd1, %rd22, -9223372036854775808;
	shr.u32 	%r20, %r3, 6;
	mov.u32 	%r21, 16;
	sub.s32 	%r4, %r21, %r20;
	mov.u64 	%rd23, 0;
	st.u64 	[%SP+72], %rd23;
	sub.s32 	%r5, %r4, 1;
	mov.u32 	%r36, %r5;

BB8_3:
	mov.u32 	%r6, %r36;
	add.s32 	%r22, %r4, 3;
	mov.u32 	%r23, 18;
	min.s32 	%r24, %r23, %r22;
	setp.lt.s32	%p3, %r6, %r24;
	not.pred 	%p4, %p3;
	@%p4 bra 	BB8_6;
	bra.uni 	BB8_4;

BB8_4:
	mov.u64 	%rd77, __cudart_i2opi_d;
	cvta.const.u64 	%rd78, %rd77;
	cvt.s64.s32	%rd79, %r6;
	shl.b64 	%rd80, %rd79, 3;
	add.s64 	%rd81, %rd78, %rd80;
	ld.u64 	%rd74, [%rd81];
	ld.u64 	%rd76, [%SP+72];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd74;    
	mov.b64         {blo,bhi}, %rd1;    
	mov.b64         {clo,chi}, %rd76;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd72, {r0,r1};      
	mov.b64         %rd73, {r2,r3};      
	}
	// inline asm
	st.u64 	[%SP+0], %rd72;
	st.u64 	[%SP+8], %rd73;
	ld.u64 	%rd82, [%SP+0];
	ld.u64 	%rd83, [%SP+8];
	st.u64 	[%SP+72], %rd83;
	st.u64 	[%SP+64], %rd82;
	ld.u64 	%rd84, [%SP+64];
	sub.s32 	%r34, %r4, 1;
	sub.s32 	%r35, %r6, %r34;
	cvt.s64.s32	%rd85, %r35;
	shl.b64 	%rd86, %rd85, 3;
	add.u64 	%rd87, %SP, 80;
	add.s64 	%rd88, %rd87, %rd86;
	st.u64 	[%rd88], %rd84;

	add.s32 	%r7, %r6, 1;
	mov.u32 	%r36, %r7;
	bra.uni 	BB8_3;

BB8_6:
	ld.u64 	%rd24, [%SP+72];
	sub.s32 	%r25, %r4, 1;
	sub.s32 	%r26, %r6, %r25;
	cvt.s64.s32	%rd25, %r26;
	shl.b64 	%rd26, %rd25, 3;
	add.u64 	%rd27, %SP, 80;
	add.s64 	%rd28, %rd27, %rd26;
	st.u64 	[%rd28], %rd24;
	and.b32  	%r8, %r3, 63;
	ld.u64 	%rd2, [%SP+96];
	ld.u64 	%rd3, [%SP+104];
	setp.ne.s32	%p5, %r8, 0;
	not.pred 	%p6, %p5;
	mov.u64 	%rd89, %rd2;
	mov.u64 	%rd90, %rd3;
	@%p6 bra 	BB8_8;
	bra.uni 	BB8_7;

BB8_7:
	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r8;
	shl.b64 	%rd29, %rd3, %r8;
	shr.u64 	%rd30, %rd2, %r28;
	or.b64  	%rd4, %rd29, %rd30;
	shl.b64 	%rd31, %rd2, %r8;
	ld.u64 	%rd32, [%SP+88];
	shr.u64 	%rd33, %rd32, %r28;
	or.b64  	%rd5, %rd31, %rd33;
	mov.u64 	%rd89, %rd5;
	mov.u64 	%rd90, %rd4;

BB8_8:
	mov.u64 	%rd7, %rd90;
	mov.u64 	%rd6, %rd89;
	shr.u64 	%rd34, %rd7, 62;
	cvt.u32.u64	%r29, %rd34;
	shl.b64 	%rd35, %rd7, 2;
	shr.u64 	%rd36, %rd6, 62;
	or.b64  	%rd8, %rd35, %rd36;
	shl.b64 	%rd9, %rd6, 2;
	shr.u64 	%rd37, %rd8, 63;
	cvt.u32.u64	%r9, %rd37;
	add.s32 	%r10, %r29, %r9;
	setp.ne.s32	%p7, %r1, 0;
	not.pred 	%p8, %p7;
	mov.u32 	%r37, %r10;
	@%p8 bra 	BB8_10;
	bra.uni 	BB8_9;

BB8_9:
	neg.s32 	%r11, %r10;
	mov.u32 	%r37, %r11;

BB8_10:
	mov.u32 	%r12, %r37;
	st.u32 	[%rd20], %r12;
	setp.ne.s32	%p9, %r9, 0;
	not.pred 	%p10, %p9;
	mov.u64 	%rd91, %rd9;
	mov.u64 	%rd92, %rd8;
	mov.u32 	%r38, %r1;
	@%p10 bra 	BB8_12;
	bra.uni 	BB8_11;

BB8_11:
	mov.u64 	%rd41, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd41;
	mov.b64         {a2,a3}, %rd41;
	mov.b64         {b0,b1}, %rd9;
	mov.b64         {b2,b3}, %rd8;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd38, {r0,r1};
	mov.b64         %rd39, {r2,r3};
	}
	// inline asm
	st.u64 	[%SP+16], %rd38;
	st.u64 	[%SP+24], %rd39;
	ld.u64 	%rd44, [%SP+16];
	ld.u64 	%rd45, [%SP+24];
	st.u64 	[%SP+72], %rd45;
	st.u64 	[%SP+64], %rd44;
	ld.u64 	%rd10, [%SP+64];
	ld.u64 	%rd11, [%SP+72];
	xor.b32  	%r13, %r1, -2147483648;
	mov.u64 	%rd91, %rd10;
	mov.u64 	%rd92, %rd11;
	mov.u32 	%r38, %r13;

BB8_12:
	mov.u32 	%r14, %r38;
	mov.u64 	%rd13, %rd92;
	mov.u64 	%rd12, %rd91;
	clz.b64 	%r15, %rd13;
	setp.ne.s32	%p11, %r15, 0;
	not.pred 	%p12, %p11;
	mov.u64 	%rd93, %rd13;
	@%p12 bra 	BB8_14;
	bra.uni 	BB8_13;

BB8_13:
	shl.b64 	%rd46, %rd13, %r15;
	mov.u32 	%r30, 64;
	sub.s32 	%r31, %r30, %r15;
	shr.u64 	%rd47, %rd12, %r31;
	or.b64  	%rd14, %rd46, %rd47;
	mov.u64 	%rd93, %rd14;

BB8_14:
	mov.u64 	%rd15, %rd93;
	mov.u64 	%rd51, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd15;   
	mov.b64         {blo,bhi}, %rd51;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd48, {r0,r1};     
	mov.b64         %rd49, {r2,r3};     
	}
	// inline asm
	st.u64 	[%SP+32], %rd48;
	st.u64 	[%SP+40], %rd49;
	ld.u64 	%rd52, [%SP+32];
	ld.u64 	%rd53, [%SP+40];
	st.u64 	[%SP+72], %rd53;
	st.u64 	[%SP+64], %rd52;
	ld.u64 	%rd16, [%SP+64];
	ld.u64 	%rd17, [%SP+72];
	setp.gt.s64	%p13, %rd17, 0;
	not.pred 	%p14, %p13;
	mov.u32 	%r39, %r15;
	mov.u64 	%rd94, %rd17;
	@%p14 bra 	BB8_16;
	bra.uni 	BB8_15;

BB8_15:
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd16;
	mov.b64         {a2,a3}, %rd17;
	mov.b64         {b0,b1}, %rd16;
	mov.b64         {b2,b3}, %rd17;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd54, {r0,r1};
	mov.b64         %rd55, {r2,r3};
	}
	// inline asm
	st.u64 	[%SP+48], %rd54;
	st.u64 	[%SP+56], %rd55;
	ld.u64 	%rd60, [%SP+48];
	ld.u64 	%rd61, [%SP+56];
	st.u64 	[%SP+72], %rd61;
	st.u64 	[%SP+64], %rd60;
	ld.u64 	%rd18, [%SP+72];
	add.s32 	%r16, %r15, 1;
	mov.u32 	%r39, %r16;
	mov.u64 	%rd94, %rd18;

BB8_16:
	mov.u64 	%rd19, %rd94;
	mov.u32 	%r17, %r39;
	cvt.u64.u32	%rd62, %r14;
	shl.b64 	%rd63, %rd62, 32;
	mov.u32 	%r32, 1022;
	sub.s32 	%r33, %r32, %r17;
	cvt.u64.u32	%rd64, %r33;
	shl.b64 	%rd65, %rd64, 52;
	add.s64 	%rd66, %rd19, 1;
	shr.u64 	%rd67, %rd66, 10;
	add.s64 	%rd68, %rd67, 1;
	shr.u64 	%rd69, %rd68, 1;
	add.s64 	%rd70, %rd65, %rd69;
	or.b64  	%rd71, %rd63, %rd70;
	mov.b64 	 %fd1, %rd71;
	mov.f64 	%fd4, %fd1;

BB8_17:
	mov.f64 	%fd2, %fd4;
	st.param.f64	[func_retval0+0], %fd2;
	ret;
}

	.file	1 "C:/Users/chen.bar/Documents/Visual Studio 2019/cuda/integrateMult/kernel.cu", 1581866639, 3761
	.file	2 "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\include\\vector_types.h", 1571905694, 13615

.section .debug_info {
 .b32 2848
 .b8 2
 .b8 0
 .b32 .debug_abbrev
 .b8 8
 .b8 1

 .b8 108
 .b8 103
 .b8 101
 .b8 110
 .b8 102
 .b8 101
 .b8 58
 .b8 32
 .b8 69
 .b8 68
 .b8 71
 .b8 32
 .b8 53
 .b8 46
 .b8 48

 .b8 0
 .b8 4
 .b8 67
 .b8 58
 .b8 47
 .b8 85
 .b8 115
 .b8 101
 .b8 114
 .b8 115
 .b8 47
 .b8 99
 .b8 104
 .b8 101
 .b8 110
 .b8 46
 .b8 98
 .b8 97
 .b8 114
 .b8 47
 .b8 68
 .b8 111
 .b8 99
 .b8 117
 .b8 109
 .b8 101
 .b8 110
 .b8 116
 .b8 115
 .b8 47
 .b8 86
 .b8 105
 .b8 115
 .b8 117
 .b8 97
 .b8 108
 .b8 32
 .b8 83
 .b8 116
 .b8 117
 .b8 100
 .b8 105
 .b8 111
 .b8 32
 .b8 50
 .b8 48
 .b8 49
 .b8 57
 .b8 47
 .b8 99
 .b8 117
 .b8 100
 .b8 97
 .b8 47
 .b8 105
 .b8 110
 .b8 116
 .b8 101
 .b8 103
 .b8 114
 .b8 97
 .b8 116
 .b8 101
 .b8 77
 .b8 117
 .b8 108
 .b8 116
 .b8 47
 .b8 107
 .b8 101
 .b8 114
 .b8 110
 .b8 101
 .b8 108
 .b8 46
 .b8 99
 .b8 117

 .b8 0
 .b64 0
 .b32 .debug_line
 .b8 67
 .b8 58
 .b8 92
 .b8 85
 .b8 115
 .b8 101
 .b8 114
 .b8 115
 .b8 92
 .b8 99
 .b8 104
 .b8 101
 .b8 110
 .b8 46
 .b8 98
 .b8 97
 .b8 114
 .b8 92
 .b8 68
 .b8 111
 .b8 99
 .b8 117
 .b8 109
 .b8 101
 .b8 110
 .b8 116
 .b8 115
 .b8 92
 .b8 86
 .b8 105
 .b8 115
 .b8 117
 .b8 97
 .b8 108
 .b8 32
 .b8 83
 .b8 116
 .b8 117
 .b8 100
 .b8 105
 .b8 111
 .b8 32
 .b8 50
 .b8 48
 .b8 49
 .b8 57
 .b8 92
 .b8 99
 .b8 117
 .b8 100
 .b8 97
 .b8 92
 .b8 105
 .b8 110
 .b8 116
 .b8 101
 .b8 103
 .b8 114
 .b8 97
 .b8 116
 .b8 101
 .b8 77
 .b8 117
 .b8 108
 .b8 116

 .b8 0
 .b8 2

 .b8 117
 .b8 68
 .b8 105
 .b8 109
 .b8 80
 .b8 114
 .b8 111
 .b8 100

 .b8 0
 .b32 219
 .b8 1
 .b8 6
 .b8 9
 .b8 3
 .b64 uDimProd
 .b8 117
 .b8 68
 .b8 105
 .b8 109
 .b8 80
 .b8 114
 .b8 111
 .b8 100

 .b8 0
 .b8 4
 .b8 3

 .b32 231
 .b8 4

 .b32 251
 .b8 2

 .b8 0
 .b8 5

 .b32 244
 .b8 105
 .b8 110
 .b8 116
 .b8 51
 .b8 50
 .b8 95
 .b8 116

 .b8 0
 .b8 6

 .b8 105
 .b8 110
 .b8 116

 .b8 0
 .b8 5
 .b8 4
 .b8 7

 .b8 105
 .b8 110
 .b8 116

 .b8 0
 .b8 4
 .b8 5
 .b8 2

 .b8 108
 .b8 68
 .b8 105
 .b8 109

 .b8 0
 .b32 286
 .b8 1
 .b8 7
 .b8 9
 .b8 3
 .b64 lDim
 .b8 108
 .b8 68
 .b8 105
 .b8 109

 .b8 0
 .b8 4
 .b8 3

 .b32 231
 .b8 4

 .b32 251
 .b8 3

 .b8 0
 .b8 2

 .b8 118
 .b8 68
 .b8 105
 .b8 109

 .b8 0
 .b32 286
 .b8 1
 .b8 8
 .b8 9
 .b8 3
 .b64 vDim
 .b8 118
 .b8 68
 .b8 105
 .b8 109

 .b8 0
 .b8 4
 .b8 2

 .b8 114
 .b8 97
 .b8 110
 .b8 100
 .b8 80
 .b8 104
 .b8 97
 .b8 115
 .b8 101

 .b8 0
 .b32 364
 .b8 1
 .b8 9
 .b8 9
 .b8 3
 .b64 randPhase
 .b8 114
 .b8 97
 .b8 110
 .b8 100
 .b8 80
 .b8 104
 .b8 97
 .b8 115
 .b8 101

 .b8 0
 .b8 4
 .b8 8

 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 16
 .b8 2
 .b16 334
 .b8 9

 .b8 120

 .b8 0
 .b32 402
 .b8 2
 .b16 336
 .b8 0
 .b8 1
 .b8 9

 .b8 121

 .b8 0
 .b32 402
 .b8 2
 .b16 336
 .b8 8
 .b8 1
 .b8 0
 .b8 6

 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101

 .b8 0
 .b8 4
 .b8 8
 .b8 2

 .b8 108
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 65
 .b8 108
 .b8 112
 .b8 104
 .b8 97

 .b8 0
 .b32 458
 .b8 1
 .b8 10
 .b8 9
 .b8 3
 .b64 lMixtureAlpha
 .b8 108
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 65
 .b8 108
 .b8 112
 .b8 104
 .b8 97

 .b8 0
 .b8 4
 .b8 3

 .b32 402
 .b8 4

 .b32 251
 .b8 31

 .b8 0
 .b8 10

 .b8 95
 .b8 90
 .b8 112
 .b8 108
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 95
 .b8 90
 .b8 112
 .b8 108
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 1
 .b8 14
 .b32 364
 .b8 1
 .b8 11

 .b8 97

 .b8 0
 .b8 1
 .b8 14
 .b32 364
 .b8 11

 .b8 98

 .b8 0
 .b8 1
 .b8 14
 .b32 364
 .b8 0
 .b8 10

 .b8 95
 .b8 90
 .b8 100
 .b8 118
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 95
 .b8 90
 .b8 100
 .b8 118
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 1
 .b8 27
 .b32 364
 .b8 1
 .b8 11

 .b8 97

 .b8 0
 .b8 1
 .b8 27
 .b32 364
 .b8 11

 .b8 98

 .b8 0
 .b8 1
 .b8 27
 .b32 364
 .b8 12

 .b8 13

 .b8 99

 .b8 0
 .b8 1
 .b8 29
 .b32 364
 .b8 13

 .b8 100
 .b8 101
 .b8 110
 .b8 111
 .b8 109
 .b8 105
 .b8 110
 .b8 97
 .b8 116
 .b8 111
 .b8 114

 .b8 0
 .b8 1
 .b8 30
 .b32 402
 .b8 0
 .b8 0
 .b8 10

 .b8 95
 .b8 90
 .b8 109
 .b8 108
 .b8 100
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 95
 .b8 90
 .b8 109
 .b8 108
 .b8 100
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 1
 .b8 36
 .b32 364
 .b8 1
 .b8 11

 .b8 97

 .b8 0
 .b8 1
 .b8 36
 .b32 402
 .b8 11

 .b8 98

 .b8 0
 .b8 1
 .b8 36
 .b32 364
 .b8 0
 .b8 10

 .b8 95
 .b8 90
 .b8 109
 .b8 108
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 95
 .b8 90
 .b8 109
 .b8 108
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 1
 .b8 43
 .b32 364
 .b8 1
 .b8 11

 .b8 97

 .b8 0
 .b8 1
 .b8 43
 .b32 364
 .b8 11

 .b8 98

 .b8 0
 .b8 1
 .b8 43
 .b32 364
 .b8 12

 .b8 13

 .b8 116
 .b8 109
 .b8 112
 .b8 86
 .b8 97
 .b8 114

 .b8 0
 .b8 1
 .b8 45
 .b32 364
 .b8 0
 .b8 0
 .b8 10

 .b8 95
 .b8 90
 .b8 49
 .b8 51
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 83
 .b8 113
 .b8 117
 .b8 97
 .b8 114
 .b8 101
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 95
 .b8 90
 .b8 49
 .b8 51
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 83
 .b8 113
 .b8 117
 .b8 97
 .b8 114
 .b8 101
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 1
 .b8 52
 .b32 364
 .b8 1
 .b8 11

 .b8 97

 .b8 0
 .b8 1
 .b8 52
 .b32 364
 .b8 12

 .b8 13

 .b8 116
 .b8 109
 .b8 112
 .b8 86
 .b8 97
 .b8 114

 .b8 0
 .b8 1
 .b8 54
 .b32 402
 .b8 0
 .b8 0
 .b8 14

 .b8 95
 .b8 90
 .b8 49
 .b8 49
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 83
 .b8 113
 .b8 114
 .b8 116
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 95
 .b8 90
 .b8 49
 .b8 49
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 83
 .b8 113
 .b8 114
 .b8 116
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 1
 .b8 60
 .b32 364
 .b8 1
 .b64 func_begin0
 .b64 func_end0
 .b8 1
 .b8 156
 .b8 15

 .b8 97

 .b8 0
 .b8 1
 .b8 60
 .b32 364
 .b8 11
 .b8 3
 .b64 __local_depot0
 .b8 35
 .b8 0

 .b8 6
 .b8 16

 .b64 tmp0
 .b64 tmp3
 .b8 17

 .b8 114

 .b8 0
 .b8 1
 .b8 62
 .b32 402
 .b8 6
 .b8 144
 .b8 181
 .b8 200
 .b8 153
 .b8 171
 .b8 2
 .b8 2
 .b8 17

 .b8 97
 .b8 98
 .b8 115
 .b8 114
 .b8 122

 .b8 0
 .b8 1
 .b8 62
 .b32 402
 .b8 7
 .b8 144
 .b8 177
 .b8 226
 .b8 144
 .b8 179
 .b8 214
 .b8 4
 .b8 2
 .b8 0
 .b8 0
 .b8 10

 .b8 95
 .b8 90
 .b8 49
 .b8 53
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 69
 .b8 120
 .b8 112
 .b8 111
 .b8 110
 .b8 101
 .b8 110
 .b8 116
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 95
 .b8 90
 .b8 49
 .b8 53
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 69
 .b8 120
 .b8 112
 .b8 111
 .b8 110
 .b8 101
 .b8 110
 .b8 116
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50

 .b8 0
 .b8 1
 .b8 71
 .b32 364
 .b8 1
 .b8 11

 .b8 97

 .b8 0
 .b8 1
 .b8 71
 .b32 364
 .b8 12

 .b8 13

 .b8 115
 .b8 105
 .b8 110
 .b8 97

 .b8 0
 .b8 1
 .b8 73
 .b32 402
 .b8 13

 .b8 99
 .b8 111
 .b8 115
 .b8 97

 .b8 0
 .b8 1
 .b8 73
 .b32 402
 .b8 13

 .b8 101
 .b8 120
 .b8 112
 .b8 114

 .b8 0
 .b8 1
 .b8 73
 .b32 402
 .b8 0
 .b8 0
 .b8 10

 .b8 95
 .b8 90
 .b8 49
 .b8 53
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 73
 .b8 110
 .b8 99
 .b8 114
 .b8 101
 .b8 97
 .b8 115
 .b8 101
 .b8 80
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 95
 .b8 90
 .b8 49
 .b8 53
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 73
 .b8 110
 .b8 99
 .b8 114
 .b8 101
 .b8 97
 .b8 115
 .b8 101
 .b8 80
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95

 .b8 0
 .b8 1
 .b8 81
 .b32 1191
 .b8 1
 .b8 11

 .b8 97
 .b8 100
 .b8 100
 .b8 114
 .b8 101
 .b8 115
 .b8 115

 .b8 0
 .b8 1
 .b8 81
 .b32 2834
 .b8 11

 .b8 118
 .b8 97
 .b8 108

 .b8 0
 .b8 1
 .b8 81
 .b32 364
 .b8 0
 .b8 18

 .b8 118
 .b8 111
 .b8 105
 .b8 100

 .b8 0
 .b8 14

 .b8 95
 .b8 90
 .b8 49
 .b8 51
 .b8 105
 .b8 110
 .b8 116
 .b8 101
 .b8 103
 .b8 114
 .b8 97
 .b8 116
 .b8 101
 .b8 77
 .b8 117
 .b8 108
 .b8 116
 .b8 80
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 80
 .b8 75
 .b8 83
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95

 .b8 0
 .b8 95
 .b8 90
 .b8 49
 .b8 51
 .b8 105
 .b8 110
 .b8 116
 .b8 101
 .b8 103
 .b8 114
 .b8 97
 .b8 116
 .b8 101
 .b8 77
 .b8 117
 .b8 108
 .b8 116
 .b8 80
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 80
 .b8 75
 .b8 83
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95

 .b8 0
 .b8 1
 .b8 89
 .b32 1191
 .b8 1
 .b64 func_begin1
 .b64 func_end1
 .b8 1
 .b8 156
 .b8 15

 .b8 117

 .b8 0
 .b8 1
 .b8 89
 .b32 2834
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_0
 .b8 7
 .b8 15

 .b8 108
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 77
 .b8 117
 .b8 49

 .b8 0
 .b8 1
 .b8 90
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_1
 .b8 7
 .b8 15

 .b8 108
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 77
 .b8 117
 .b8 50

 .b8 0
 .b8 1
 .b8 90
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_2
 .b8 7
 .b8 15

 .b8 108
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 77
 .b8 117
 .b8 51

 .b8 0
 .b8 1
 .b8 90
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_3
 .b8 7
 .b8 15

 .b8 108
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 67

 .b8 0
 .b8 1
 .b8 90
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_4
 .b8 7
 .b8 15

 .b8 118
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 77
 .b8 117
 .b8 49

 .b8 0
 .b8 1
 .b8 91
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_5
 .b8 7
 .b8 15

 .b8 118
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 77
 .b8 117
 .b8 50

 .b8 0
 .b8 1
 .b8 91
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_6
 .b8 7
 .b8 15

 .b8 118
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 77
 .b8 117
 .b8 51

 .b8 0
 .b8 1
 .b8 91
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_7
 .b8 7
 .b8 15

 .b8 118
 .b8 77
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 67

 .b8 0
 .b8 1
 .b8 91
 .b32 2840
 .b8 9
 .b8 3
 .b64 _Z13integrateMultP7double2PKS_S2_S2_S2_S2_S2_S2_S2__param_8
 .b8 7
 .b8 16

 .b64 tmp4
 .b64 tmp57
 .b8 17

 .b8 117
 .b8 73
 .b8 100
 .b8 120

 .b8 0
 .b8 1
 .b8 93
 .b32 244
 .b8 5
 .b8 144
 .b8 177
 .b8 228
 .b8 149
 .b8 1
 .b8 2
 .b8 16

 .b64 tmp6
 .b64 tmp56
 .b8 17

 .b8 117
 .b8 68
 .b8 105
 .b8 109
 .b8 73
 .b8 100
 .b8 120

 .b8 0
 .b8 1
 .b8 96
 .b32 2791
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 176
 .b8 3

 .b8 6
 .b8 17

 .b8 115
 .b8 113
 .b8 114
 .b8 116
 .b8 77
 .b8 117

 .b8 0
 .b8 1
 .b8 102
 .b32 364
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 192
 .b8 3

 .b8 6
 .b8 17

 .b8 118
 .b8 77
 .b8 117
 .b8 49

 .b8 0
 .b8 1
 .b8 104
 .b32 364
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 208
 .b8 3

 .b8 6
 .b8 17

 .b8 118
 .b8 77
 .b8 117
 .b8 50

 .b8 0
 .b8 1
 .b8 105
 .b32 364
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 224
 .b8 3

 .b8 6
 .b8 17

 .b8 118
 .b8 77
 .b8 117
 .b8 51

 .b8 0
 .b8 1
 .b8 106
 .b32 364
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 240
 .b8 3

 .b8 6
 .b8 17

 .b8 118
 .b8 67

 .b8 0
 .b8 1
 .b8 107
 .b32 364
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 128
 .b8 4

 .b8 6
 .b8 17

 .b8 108
 .b8 73
 .b8 100
 .b8 120

 .b8 0
 .b8 1
 .b8 100
 .b32 244
 .b8 5
 .b8 144
 .b8 178
 .b8 228
 .b8 149
 .b8 1
 .b8 2
 .b8 17

 .b8 118
 .b8 73
 .b8 100
 .b8 120

 .b8 0
 .b8 1
 .b8 101
 .b32 244
 .b8 6
 .b8 144
 .b8 181
 .b8 236
 .b8 200
 .b8 171
 .b8 2
 .b8 2
 .b8 17

 .b8 99
 .b8 117
 .b8 114
 .b8 114
 .b8 108
 .b8 73
 .b8 100
 .b8 120

 .b8 0
 .b8 1
 .b8 103
 .b32 244
 .b8 7
 .b8 144
 .b8 176
 .b8 224
 .b8 196
 .b8 145
 .b8 215
 .b8 4
 .b8 2
 .b8 16

 .b64 tmp9
 .b64 tmp56
 .b8 19

 .b8 109
 .b8 105
 .b8 120
 .b8 116
 .b8 117
 .b8 114
 .b8 101
 .b8 73
 .b8 100
 .b8 120

 .b8 0
 .b8 1
 .b8 109
 .b32 244
 .b32 .debug_loc
 .b8 16

 .b64 tmp13
 .b64 tmp56
 .b8 16

 .b64 tmp13
 .b64 tmp53
 .b8 20

 .b32 470
 .b64 tmp15
 .b64 tmp16
 .b8 1
 .b8 112
 .b8 21

 .b32 508
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 144
 .b8 3

 .b8 6
 .b8 21

 .b32 517
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 160
 .b8 3

 .b8 6
 .b8 0
 .b8 22

 .b32 742
 .b32 .debug_ranges
 .b8 1
 .b8 112
 .b8 21

 .b32 802
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 192
 .b8 2

 .b8 6
 .b8 21

 .b32 508
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 192
 .b8 1

 .b8 6
 .b8 21

 .b32 508
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 80

 .b8 6
 .b8 21

 .b32 517
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 208
 .b8 1

 .b8 6
 .b8 21

 .b32 517
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 96

 .b8 6
 .b8 23

 .b32 812
 .b8 7
 .b8 144
 .b8 180
 .b8 226
 .b8 144
 .b8 179
 .b8 214
 .b8 4
 .b8 2
 .b8 0
 .b8 20

 .b32 470
 .b64 tmp20
 .b64 tmp21
 .b8 1
 .b8 113
 .b8 21

 .b32 508
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 160
 .b8 2

 .b8 6
 .b8 21

 .b32 517
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 176
 .b8 2

 .b8 6
 .b8 0
 .b8 20

 .b32 742
 .b64 tmp22
 .b64 tmp24
 .b8 1
 .b8 113
 .b8 21

 .b32 802
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 224
 .b8 1

 .b8 6
 .b8 23

 .b32 812
 .b8 7
 .b8 144
 .b8 176
 .b8 232
 .b8 144
 .b8 179
 .b8 214
 .b8 4
 .b8 2
 .b8 0
 .b8 20

 .b32 470
 .b64 tmp27
 .b64 tmp28
 .b8 1
 .b8 114
 .b8 21

 .b32 508
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 160
 .b8 1

 .b8 6
 .b8 21

 .b32 517
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 176
 .b8 1

 .b8 6
 .b8 0
 .b8 20

 .b32 742
 .b64 tmp29
 .b64 tmp31
 .b8 1
 .b8 114
 .b8 21

 .b32 802
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 112

 .b8 6
 .b8 23

 .b32 812
 .b8 7
 .b8 144
 .b8 180
 .b8 238
 .b8 144
 .b8 179
 .b8 214
 .b8 4
 .b8 2
 .b8 0
 .b8 20

 .b32 614
 .b64 tmp36
 .b64 tmp37
 .b8 1
 .b8 115
 .b8 21

 .b32 650
 .b8 8
 .b8 144
 .b8 176
 .b8 224
 .b8 196
 .b8 161
 .b8 230
 .b8 172
 .b8 9
 .b8 2
 .b8 21

 .b32 659
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 0

 .b8 6
 .b8 0
 .b8 22

 .b32 470
 .b32 .debug_ranges+64
 .b8 1
 .b8 115
 .b8 21

 .b32 508
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 16

 .b8 6
 .b8 21

 .b32 508
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 48

 .b8 6
 .b8 21

 .b32 517
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 32

 .b8 6
 .b8 21

 .b32 517
 .b8 11
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 64

 .b8 6
 .b8 0
 .b8 22

 .b32 982
 .b32 .debug_ranges+112
 .b8 1
 .b8 115
 .b8 21

 .b32 1046
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 128
 .b8 1

 .b8 6
 .b8 21

 .b32 565
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 240
 .b8 1

 .b8 6
 .b8 21

 .b32 574
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 128
 .b8 2

 .b8 6
 .b8 23

 .b32 1056
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 144
 .b8 1

 .b8 6
 .b8 23

 .b32 1068
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 152
 .b8 1

 .b8 6
 .b8 23

 .b32 584
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 144
 .b8 2

 .b8 6
 .b8 23

 .b32 1080
 .b8 8
 .b8 144
 .b8 176
 .b8 230
 .b8 196
 .b8 161
 .b8 230
 .b8 172
 .b8 9
 .b8 2
 .b8 23

 .b32 593
 .b8 8
 .b8 144
 .b8 182
 .b8 232
 .b8 196
 .b8 161
 .b8 230
 .b8 172
 .b8 9
 .b8 2
 .b8 0
 .b8 20

 .b32 669
 .b64 tmp50
 .b64 tmp51
 .b8 1
 .b8 115
 .b8 21

 .b32 707
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 208
 .b8 2

 .b8 6
 .b8 21

 .b32 716
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 224
 .b8 2

 .b8 6
 .b8 23

 .b32 726
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 240
 .b8 2

 .b8 6
 .b8 0
 .b8 20

 .b32 1094
 .b64 tmp52
 .b64 tmp53
 .b8 1
 .b8 115
 .b8 21

 .b32 1164
 .b8 7
 .b8 144
 .b8 179
 .b8 232
 .b8 144
 .b8 147
 .b8 215
 .b8 4
 .b8 2
 .b8 21

 .b32 1179
 .b8 12
 .b8 3
 .b64 __local_depot1
 .b8 35
 .b8 128
 .b8 3

 .b8 6
 .b8 0
 .b8 0
 .b8 0
 .b8 0
 .b8 0
 .b8 0
 .b8 0
 .b8 24

 .b8 105
 .b8 110
 .b8 116
 .b8 51

 .b8 0
 .b8 12
 .b8 2
 .b8 185
 .b8 25

 .b8 120

 .b8 0
 .b32 244
 .b8 2
 .b8 187
 .b8 0
 .b8 1
 .b8 25

 .b8 121

 .b8 0
 .b32 244
 .b8 2
 .b8 187
 .b8 4
 .b8 1
 .b8 25

 .b8 122

 .b8 0
 .b32 244
 .b8 2
 .b8 187
 .b8 8
 .b8 1
 .b8 0
 .b8 26

 .b32 364
 .b8 12
 .b8 26

 .b32 2846
 .b8 12
 .b8 27

 .b32 364
 .b8 0
}
.section .debug_abbrev {
 .b8 1

 .b8 17

 .b8 1

 .b8 37

 .b8 8

 .b8 19

 .b8 11

 .b8 3

 .b8 8

 .b8 17

 .b8 1

 .b8 16

 .b8 6

 .b8 27

 .b8 8

 .b8 0

 .b8 0

 .b8 2

 .b8 52

 .b8 0

 .b8 3

 .b8 8

 .b8 73

 .b8 19

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 2

 .b8 10

 .b8 135
 .b8 64

 .b8 8

 .b8 51

 .b8 11

 .b8 0

 .b8 0

 .b8 3

 .b8 1

 .b8 1

 .b8 73

 .b8 19

 .b8 0

 .b8 0

 .b8 4

 .b8 33

 .b8 0

 .b8 73

 .b8 19

 .b8 47

 .b8 15

 .b8 0

 .b8 0

 .b8 5

 .b8 22

 .b8 0

 .b8 73

 .b8 19

 .b8 3

 .b8 8

 .b8 0

 .b8 0

 .b8 6

 .b8 36

 .b8 0

 .b8 3

 .b8 8

 .b8 62

 .b8 11

 .b8 11

 .b8 11

 .b8 0

 .b8 0

 .b8 7

 .b8 36

 .b8 0

 .b8 3

 .b8 8

 .b8 11

 .b8 11

 .b8 62

 .b8 11

 .b8 0

 .b8 0

 .b8 8

 .b8 19

 .b8 1

 .b8 3

 .b8 8

 .b8 11

 .b8 11

 .b8 58

 .b8 11

 .b8 59

 .b8 5

 .b8 0

 .b8 0

 .b8 9

 .b8 13

 .b8 0

 .b8 3

 .b8 8

 .b8 73

 .b8 19

 .b8 58

 .b8 11

 .b8 59

 .b8 5

 .b8 56

 .b8 11

 .b8 50

 .b8 11

 .b8 0

 .b8 0

 .b8 10

 .b8 46

 .b8 1

 .b8 135
 .b8 64

 .b8 8

 .b8 3

 .b8 8

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 73

 .b8 19

 .b8 32

 .b8 11

 .b8 0

 .b8 0

 .b8 11

 .b8 5

 .b8 0

 .b8 3

 .b8 8

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 73

 .b8 19

 .b8 0

 .b8 0

 .b8 12

 .b8 11

 .b8 1

 .b8 0

 .b8 0

 .b8 13

 .b8 52

 .b8 0

 .b8 3

 .b8 8

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 73

 .b8 19

 .b8 0

 .b8 0

 .b8 14

 .b8 46

 .b8 1

 .b8 135
 .b8 64

 .b8 8

 .b8 3

 .b8 8

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 73

 .b8 19

 .b8 63

 .b8 12

 .b8 17

 .b8 1

 .b8 18

 .b8 1

 .b8 64

 .b8 10

 .b8 0

 .b8 0

 .b8 15

 .b8 5

 .b8 0

 .b8 3

 .b8 8

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 73

 .b8 19

 .b8 2

 .b8 10

 .b8 51

 .b8 11

 .b8 0

 .b8 0

 .b8 16

 .b8 11

 .b8 1

 .b8 17

 .b8 1

 .b8 18

 .b8 1

 .b8 0

 .b8 0

 .b8 17

 .b8 52

 .b8 0

 .b8 3

 .b8 8

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 73

 .b8 19

 .b8 2

 .b8 10

 .b8 51

 .b8 11

 .b8 0

 .b8 0

 .b8 18

 .b8 59

 .b8 0

 .b8 3

 .b8 8

 .b8 0

 .b8 0

 .b8 19

 .b8 52

 .b8 0

 .b8 3

 .b8 8

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 73

 .b8 19

 .b8 2

 .b8 6

 .b8 0

 .b8 0

 .b8 20

 .b8 29

 .b8 1

 .b8 49

 .b8 19

 .b8 17

 .b8 1

 .b8 18

 .b8 1

 .b8 88

 .b8 11

 .b8 89

 .b8 11

 .b8 0

 .b8 0

 .b8 21

 .b8 5

 .b8 0

 .b8 49

 .b8 19

 .b8 2

 .b8 10

 .b8 51

 .b8 11

 .b8 0

 .b8 0

 .b8 22

 .b8 29

 .b8 1

 .b8 49

 .b8 19

 .b8 85

 .b8 6

 .b8 88

 .b8 11

 .b8 89

 .b8 11

 .b8 0

 .b8 0

 .b8 23

 .b8 52

 .b8 0

 .b8 49

 .b8 19

 .b8 2

 .b8 10

 .b8 51

 .b8 11

 .b8 0

 .b8 0

 .b8 24

 .b8 19

 .b8 1

 .b8 3

 .b8 8

 .b8 11

 .b8 11

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 0

 .b8 0

 .b8 25

 .b8 13

 .b8 0

 .b8 3

 .b8 8

 .b8 73

 .b8 19

 .b8 58

 .b8 11

 .b8 59

 .b8 11

 .b8 56

 .b8 11

 .b8 50

 .b8 11

 .b8 0

 .b8 0

 .b8 26

 .b8 15

 .b8 0

 .b8 73

 .b8 19

 .b8 51

 .b8 11

 .b8 0

 .b8 0

 .b8 27

 .b8 38

 .b8 0

 .b8 73

 .b8 19

 .b8 0

 .b8 0

 .b8 0

}
.section .debug_loc {
 .b64 tmp10
 .b64 tmp11
 .b8 5
 .b8 0
 .b8 144
 .b8 179
 .b8 228
 .b8 149
 .b8 1
 .b64 tmp11
 .b64 tmp12
 .b8 7
 .b8 0
 .b8 144
 .b8 185
 .b8 224
 .b8 196
 .b8 145
 .b8 215
 .b8 4
 .b64 tmp12
 .b64 tmp54
 .b8 5
 .b8 0
 .b8 144
 .b8 180
 .b8 228
 .b8 149
 .b8 1
 .b64 tmp54
 .b64 tmp55
 .b8 5
 .b8 0
 .b8 144
 .b8 181
 .b8 228
 .b8 149
 .b8 1
 .b64 tmp55
 .b64 func_end1
 .b8 7
 .b8 0
 .b8 144
 .b8 185
 .b8 224
 .b8 196
 .b8 145
 .b8 215
 .b8 4
 .b64 0
 .b64 0
}
.section .debug_ranges {
 .b64 tmp17
 .b64 tmp19
 .b64 tmp25
 .b64 tmp26
 .b64 tmp32
 .b64 tmp33
 .b64 0
 .b64 0
 .b64 tmp38
 .b64 tmp39
 .b64 tmp40
 .b64 tmp41
 .b64 0
 .b64 0
 .b64 tmp42
 .b64 tmp46
 .b64 tmp47
 .b64 tmp49
 .b64 0
 .b64 0
}
.section .debug_pubnames {
 .b32 270
 .b8 2
 .b8 0
 .b32 .debug_info
 .b32 2848
 .b32 828
 .b8 95
 .b8 90
 .b8 49
 .b8 49
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 83
 .b8 113
 .b8 114
 .b8 116
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 0

 .b32 1094
 .b8 95
 .b8 90
 .b8 49
 .b8 53
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 73
 .b8 110
 .b8 99
 .b8 114
 .b8 101
 .b8 97
 .b8 115
 .b8 101
 .b8 80
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95
 .b8 0

 .b32 669
 .b8 95
 .b8 90
 .b8 109
 .b8 108
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95
 .b8 0

 .b32 527
 .b8 95
 .b8 90
 .b8 100
 .b8 118
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95
 .b8 0

 .b32 614
 .b8 95
 .b8 90
 .b8 109
 .b8 108
 .b8 100
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 0

 .b32 470
 .b8 95
 .b8 90
 .b8 112
 .b8 108
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 83
 .b8 95
 .b8 0

 .b32 982
 .b8 95
 .b8 90
 .b8 49
 .b8 53
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 69
 .b8 120
 .b8 112
 .b8 111
 .b8 110
 .b8 101
 .b8 110
 .b8 116
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 0

 .b32 742
 .b8 95
 .b8 90
 .b8 49
 .b8 51
 .b8 99
 .b8 111
 .b8 109
 .b8 112
 .b8 108
 .b8 101
 .b8 120
 .b8 83
 .b8 113
 .b8 117
 .b8 97
 .b8 114
 .b8 101
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 0

 .b32 1197
 .b8 95
 .b8 90
 .b8 49
 .b8 51
 .b8 105
 .b8 110
 .b8 116
 .b8 101
 .b8 103
 .b8 114
 .b8 97
 .b8 116
 .b8 101
 .b8 77
 .b8 117
 .b8 108
 .b8 116
 .b8 80
 .b8 55
 .b8 100
 .b8 111
 .b8 117
 .b8 98
 .b8 108
 .b8 101
 .b8 50
 .b8 80
 .b8 75
 .b8 83
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 83
 .b8 50
 .b8 95
 .b8 0

 .b32 0
}
